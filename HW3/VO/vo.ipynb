{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oltMOCo4gkD0"
      },
      "source": [
        "Visual Odometry (VO)\n",
        "\n",
        "You will use the pykitti module and KITTI odometry dataset.\n",
        "\n",
        "Download the odometry data from [here](https://drive.google.com/file/d/1Vbom0TPDB-NIkqrqsfUuGvDMi2S08uEt/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffiRr-EEgkD9"
      },
      "source": [
        "## Monocular VO with OpenCV on KITTI\n",
        "\n",
        "For each consecutive frame pair in the sequence, you will compute the relative pose between the frames and visualize it. You will use:\n",
        "\n",
        "* pykitti code similar to what you wrote in Week 3 to load the seqeunce with ground-truth info. (Check out the [demo code](https://github.com/utiasSTARS/pykitti/blob/master/demos/demo_odometry.py))\n",
        "* OpenCV functions to compute and visualize the features and the essential matrix.\n",
        "\n",
        "Please follow these steps to complete the assignment:\n",
        "\n",
        "1. You can use the ORB Feature to do the feature matching:\n",
        "    `orb = cv2.ORB_create()` to create the ORB object\n",
        "    and then `orb.detectAndCompute()` to find the keypoints and descriptors on both frames\n",
        "\n",
        "2. You can use brute-force matcher to match ORB descriptors:\n",
        "    `bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)`\n",
        "\n",
        "3. After matching the descriptors, sort the matched keypoints.\n",
        "\n",
        "4. Draw matches on the two images using the `cv2.drawMatches()` function.\n",
        "\n",
        "5. Compute the essential matrix using the `cv2.findEssentialMat()` function. Note that you need the matching points and the instrinsics for this function. \n",
        "\n",
        "6. Extract the rotation and translation from the essential matrix using the `cv2.recoverPose()` function.\n",
        "\n",
        "7. Multiply the estimated rotation and translation with the previous rotation and translation. Initialize rotation to identity and translation to zeros on the first frame.\n",
        "\n",
        "8. Display the current image with the keypoints on it using the `cv2.drawKeypoints()` function.\n",
        "\n",
        "9. Update the previous rotation and translation as the current rotation and translation.\n",
        "\n",
        "10. Draw the estimated trajectory as blue and ground-truth trajectory as green. You can use the `cv2.circle()` function.\n",
        "\n",
        "\n",
        "You can create a video of your visualization of images and poses for the provided sequence.\n",
        "\n",
        "\n",
        "Some examples repositories that might be useful:\n",
        "* https://bitbucket.org/castacks/visual_odometry_tutorial/src/master/visual-odometry/\n",
        "* https://github.com/uoip/monoVO-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ6fan382lOl"
      },
      "outputs": [],
      "source": [
        "# !cd KITTI_odometry\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz-2gNy02lOm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(r'/content/drive/MyDrive/monocular-vo/KITTI_odometry/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "eyPk3CeJChjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abFLqjwg2lOm"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "os.listdir('sequences/09')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aca_RZz2lOn"
      },
      "outputs": [],
      "source": [
        "path_dir = 'sequences/09/image_2'\n",
        "paths = [p for p in os.listdir(path_dir) if p.endswith('.png')]\n",
        "paths = sorted(paths)\n",
        "\n",
        "# path \n",
        "print(len(paths))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pwd\n",
        "# paths"
      ],
      "metadata": {
        "id": "6FjewMfEJIPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3B4W6w12lOn"
      },
      "outputs": [],
      "source": [
        "# getting these info from the demo code - hyperparams for KITTI dataset\n",
        "width = 1241.0\n",
        "height = 376.0\n",
        "fx, fy, cx, cy = [718.8560, 718.8560, 607.1928, 185.2157]\n",
        "trajMap = np.zeros((1000, 1000, 3), dtype=np.uint8)\n",
        "\n",
        "\n",
        "# gt trajectories \n",
        "gt_Traj = []\n",
        "with open('poses/09.txt') as f:\n",
        "    for line in f:\n",
        "        arr = list(map(float, line.split(' ')))\n",
        "        gt_Traj.append(np.array(arr).reshape(3, 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gt_Traj[0]"
      ],
      "metadata": {
        "id": "ELs1T51BKa4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLXWeqJ02lOn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# os.path.join(path_dir,paths[0])\n",
        "# paths"
      ],
      "metadata": {
        "id": "iVUG3vI5P4BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow # if you are running on the collab if not use cv2.show()\n",
        "# I use one of the codes suggested in the tutorials.  \n",
        "import cv2\n",
        "prev_img = cv2.imread(os.path.join(path_dir, paths[0]), 0)\n",
        "for i, path in enumerate(paths[1:]):\n",
        "    cur_img = cv2.imread(os.path.join(path_dir, path), 0)\n",
        "    # feature matching according to the demo code \n",
        "    orb = cv2.ORB_create(nfeatures=6000)\n",
        "\n",
        "    kp1, des1 = orb.detectAndCompute(prev_img, None)\n",
        "    kp2, des2 = orb.detectAndCompute(cur_img, None)\n",
        "    # use brute-force matcher\n",
        "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "    matches = bf.match(des1, des2)\n",
        "    matches = sorted(matches, key = lambda x:x.distance)\n",
        "\n",
        "\n",
        "    img_matching = cv2.drawMatches(cur_img, kp1, prev_img, kp2, matches[0:100], None)\n",
        "    # cv2.imshow('feature matching', img_matching) # if u are running on Jupyter\n",
        "    cv2_imshow(img_matching) # if you are running on colab\n",
        "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
        "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
        "    # calculating the Essential matrix \n",
        "    E, mask = cv2.findEssentialMat(pts1, pts2, focal=fx, pp=(cx, cy), method=cv2.RANSAC, prob=0.999, threshold=1)\n",
        "    pts1 = pts1[mask.ravel() == 1]\n",
        "    pts2 = pts2[mask.ravel() == 1]\n",
        "\n",
        "    _, R, t, mask = cv2.recoverPose(E, pts1, pts2, focal=fx, pp=(cx, cy))\n",
        "\n",
        "    # get camera motion\n",
        "    R = R.transpose()\n",
        "    t = np.matmul(R, t)\n",
        "\n",
        "    if i ==0 : \n",
        "        curr_R, curr_t = R, t\n",
        "    else:\n",
        "        curr_R, curr_t = np.matmul(prev_R, R), np.matmul(prev_R, t) + prev_t\n",
        "\n",
        "    # draw the current image with keypoints\n",
        "    curr_img_kp = cv2.drawKeypoints(cur_img, kp2, None, color=(0, 255, 0), flags=0)\n",
        "    # cv2.imshow('keypoints from current image', curr_img_kp) # if u are running on Jupyter\n",
        "    cv2_imshow(curr_img_kp) # if you are running on colab \n",
        "    # draw estimated trajectory (blue) and gt trajectory (red)\n",
        "    offset_draw = (int(1000/2))\n",
        "    cv2.circle(trajMap, (-int(curr_t[0])+offset_draw, int(curr_t[2])+offset_draw), 1, (255,0,0), 2)\n",
        "    cv2.circle(trajMap, (int(gt_Traj[i][0, 3])+offset_draw, -int(gt_Traj[i][2, 3])+offset_draw), 1, (0,0,255), 2)\n",
        "    # cv2.imshow('Trajectory', trajMap) # if u are running on Jupyter\n",
        "    cv2_imshow(trajMap) # if you are running on colab \n",
        "    cv2.waitKey(1)\n",
        "\n",
        "    prev_R, prev_t = curr_R, curr_t\n",
        "    prev_img = cur_img\n",
        "    \n",
        "cv2.imwrite('trajMap.png', trajMap)"
      ],
      "metadata": {
        "id": "VxUJD9Pu9lbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}